{"cells":[{"cell_type":"markdown","metadata":{},"source":["1. Crie uma MLP genérica para modelo de classificação binária (ou não) implementando forward, backward e loss.\n","    \n","    a. É necessário que o usuário possa definir quantas layers e neurônios desejar\n","    \n","    b. É necessário que a loss utilizada seja binary cross-entropy (estude um pouco mais sobre esse tópico)\n","    \n","    c. É necessário que a rede funcione para diferentes datasets de classificação, considerando que o dado de entrada para o modelo já esteja no formato desejado, como por exemplo número de features, target binário.\n","    \n","    d. É necessário que a  função de ativação da layer de output retorne uma probabilidade (entre 0 e 1), então use algo como sigmoid por exemplo.\n","    \n","    e. A regra de update dos pesos pode ser feita utilizando a regra básica do gradient descent (weight = weight - learning_rate * gradient)\n","2. (Opcional - Difícil 1) Crie uma MLP para classificar o dataset MNIST e faça as adaptações necessárias\n","3. (Opcional - Difícil 2) Crie um otimizador para update dos pesos utilizando o gradient e learning_rate. Como por exemplo, o otimizador Adam.\n","As informações da aula de hoje estão dentro do repositório de lectures/users/luisa/NN/XOR-zero.ipynb. Se precisarem das informações escritas posso emprestar meus rascunhos.\n"]},{"cell_type":"markdown","metadata":{"id":"WqufJgs-3438"},"source":["Defining generic Layer class"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1715311800061,"user":{"displayName":"Guilherme Alves Carvalho","userId":"11601573760906441130"},"user_tz":180},"id":"I_DnSbwZ3438"},"outputs":[],"source":["import numpy as np\n","import math\n","\n","class NetOperation():\n","\n","  def forward():\n","    pass\n","\n","  def backward():\n","    pass\n","\n","  def optmize():\n","    pass\n","\n","class Layer(NetOperation):\n","  def __init__(self, n_in: int, n_out: int):\n","    self.weights = np.random.randn(n_in, n_out) * 0.01\n","    self.bias = np.zeros(shape=(1, n_out))\n","\n","  # z = a1w1 + a2w2 + a3w3 + ... + b\n","  def forward(self, a):\n","    self.a = a\n","    return a.dot(self.weights) + self.bias\n","\n","  # dgrad = dL / z\n","  # dL / dwi = dL / dz * dz / dwi\n","  def backward(self, grad):\n","    self.grad_weights = self.a.T.dot(grad)\n","    self.grad_bias = np.sum(grad, axis= 0)\n","    # DUVIDA PRIMORDIAL (PRECISO VER ISSO, NÃO ENTENDI MUITO BEM)\n","    # peguei do código da luiza a ideia\n","    return grad.dot(self.weights.T)\n","\n","  def optimize(self, learning_rate):\n","    self.weights -= self.grad_weights * learning_rate\n","    self.grad_weights = None\n","\n","    self.bias -= self.grad_bias * learning_rate\n","    self.grad_bias = None\n","\n","class ReLU(NetOperation):\n","  # a: activation\n","  # z: weighted sum\n","\n","  # a = f(z)\n","  def f(z):\n","    return np.maximum(0, z)\n","  \n","  # da / dz\n","  # a = df(z)\n","  def df(z):\n","    return np.greater(0, z).astype(int)\n","\n","  def forward(self, z):\n","    self.z = z\n","    return self.f(z)\n","  \n","  # dgrad = dL / da\n","  # dL / dz = da / dz * dL / da\n","  def backward(self, grad):\n","    return self.df(self.z) * grad\n","  \n","class Sigmoid(NetOperation):\n","  # a: activation\n","  # z: weighted sum\n","\n","  def f(z):\n","    return 1 / (1 + math.exp(-z))\n","  \n","  def df(self, z):\n","    s = self.f(z)\n","    return s * (1 - s)\n","\n","  def forward(self, z):\n","    self.z = z\n","    return self.f(z)\n","  \n","  def backward(self, grad):\n","    return self.df(self.z) * grad\n","    "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class MLP(NetOperation):\n","  layers: list[NetOperation]\n","\n","  def __init__(self, n_features: int, neurons_per_layer: list[int]):\n","    input_layer = [Layer(n_features, neurons_per_layer[0])]\n","    hidden_layers = []\n","\n","    for i in range(len(neurons_per_layer)):\n","    \n","      hidden_layers.extend([ReLU(), Layer(neurons_per_layer[i], neurons_per_layer[i + 1])])\n","\n","\n","    self.layers = input_layer + hidden_layers + [Sigmoid()]\n","\n","\n","  def forward(self, x):\n","    activation = x\n","\n","    for layer in self.layers:\n","      activation = layer.forward(activation)\n","\n","    return activation\n","  \n","  # [Sigmoid(), Layer, ReLU, Layer, ReLU, Layer]\n","  # propagate error\n","  def backward(self, grad):\n","    self.layers.reverse()\n","    \n","    for layer in self.layers:\n","      grad = layer.backward(grad)\n","\n","    self.layers.reverse()\n"]},{"cell_type":"markdown","metadata":{"id":"gwbJDH8H3438"},"source":["Train"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1198,"status":"ok","timestamp":1715311801511,"user":{"displayName":"Guilherme Alves Carvalho","userId":"11601573760906441130"},"user_tz":180},"id":"3VsAU9n-3439","outputId":"15e96d7d-3d65-4259-adb3-af8d62c73a61"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0\n"]},{"ename":"NameError","evalue":"name 'train' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m hits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m     20\u001b[0m   train_sample \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     21\u001b[0m   target_sample \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"]}],"source":["from matplotlib import pyplot as plt\n","\n","def BCELoss(prediction, target):\n","  n = len(target)\n","  return - 1 / n * (target * np.log(prediction) + (1 - target) * np.log(1 - prediction))\n","\n","\n","X = np.array([\n","  [0, 0],\n","  [0, 1],\n","  [1, 0],\n","  [1, 1],\n","])\n","Y = np.array([\n","  [0],\n","  [1],\n","  [1],\n","  [0]\n","])\n","\n","net = MLP(2, [2, 2, 1])\n","\n","learning_rate = 0.1\n","batch_size = 2\n","xs = range(30)\n","accuracies = []\n","for epoch in xs:\n","  print(f'Epoch {epoch}')\n","  hits = 0\n","  for i in range(0, len(X), batch_size):\n","    train_sample = X[i]\n","    target_sample = Y[i]\n","\n","\n","    prediction = net.forward(train_sample)\n","    error = (prediction - target_sample)**2\n","\n","    net.backward(2 * (prediction - target_sample))\n","    net.optimize(learning_rate)\n","\n","    hits += prediction == target_sample\n","\n","  accuracies.append((hits / len(X)).reshape(-1)[0])\n","\n","  print(f'Accuracy: {hits / len(X)}')\n","plt.plot(xs, accuracies)\n"]},{"cell_type":"markdown","metadata":{"id":"pK33agnz3439"},"source":["Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715311801511,"user":{"displayName":"Guilherme Alves Carvalho","userId":"11601573760906441130"},"user_tz":180},"id":"B2QF_yiN3439","outputId":"b771106e-8f9a-47e8-c3ea-9f609de0c2be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  [[1.]]\n"]}],"source":["hits = 0\n","for i in range(len(test['data'])):\n","  test_sample = test['data'][i].reshape(1, 4)\n","  target_sample = test['target'][i]\n","\n","  prediction = layer.forward(test_sample)\n","  prediction = prediction >= 0.5\n","\n","  hits += prediction == target_sample\n","print('Accuracy: ', hits/len(test['data']))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
